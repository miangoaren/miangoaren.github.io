---
title: "Post 20: Rumbo a la Inteligencia artificial general 游"
collection: ai
permalink: /ai/00020_agi
date: 2023-12-15
---

&nbsp;

Me parece loco que actualmente uno de los temas en ciencia con m치s foco sea el como "ense침arle" a las maquinas a pensar. 쮺omo llegamos aqu칤?  

Personalmente, y quiz치s por ser bi칩logo, no creo que logremos crear una IA general en los siguientes 10 a침os como muchos apasionados de IA apuestan (y eso que tambi칠n soy apasionado). Aunque el inter칠s es tal que laboratorios como OpenAI contratan ingenieros con salarios de ~1.2 millones de MXN mensuales para investigar como ense침arle a pensar a las maquinas mediante metodos a los que han llamado "alineamiento", llegando incluso a usar otras IAs para alinear a las mismas IAs. 

![img](/images/ai/00020_agi.jpg)

Como aprendemos nosotros en relaci칩n con las IAs?. Por ejemplo, las IAs reciben 5 veces m치s informaci칩n que un ni침o de 5 a침os y aun as칤 las IAs no pueden alcanzar su capacidad de cognici칩n (evidentemente).  Esto se debe a que nosotros tenemos conceptos previos, constantemente estamos recibiendo se침ales a traves de nuestros sentidos, y estas se침ales tienen asociados multiples significados; como el chocolate que al pensarlo recuerdas su sabor, olor, textura, contextos, etc (algo que en IA se le llama multimodalidad)
Y haciendo estimaciones de la cantidad de datos usados para entrenar IAs vs el vocablo que vamos aprendiendo a lo largo de nuestras vidas es que llegaron esta grafica (eje Y = cantidad de informaci칩n, X = A침os), donde se ve que IAs como esta llamada Chinchilla (que es una de las m치s optimizadas) consume mucha informaci칩n y aun asi, ni a un ni침o se acerca.

![img](/images/ai/00020_agi2.jpg)

Refs:
* [Bridging the data gap between children and large language models](https://osf.io/preprints/psyarxiv/qzbgx)
* [BUn poco sobre el trabajo de OpenAI](https://openai.com/research/weak-to-strong-generalization)
* [BY un bonito seminario sobre la evoluci칩n de la conciencia](https://www.youtube.com/watch?v=9QWaZp_2I1k)


